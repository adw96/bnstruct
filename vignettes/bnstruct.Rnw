\documentclass{article}
% \VignetteIndexEntry{R package for Bayesian Network Structure Learning}
% \VignettePackage{bnstruct}
% \VignetteKeywords{Documentation}
\usepackage{url}
\usepackage[numbers]{natbib}\citeindextrue

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\texttt{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}

\begin{document}
\SweaveOpts{concordance=TRUE}

\title{R package for Bayesian Network Structure Learning}
\author{Francesco Sambo, Alberto Franzin}
\maketitle

<<echo=FALSE,print=false,include=FALSE,results=hide>>=
library(bnstruct)
@

\section{Introduction}
Bayesian Networks are a powerful tool for probabilistic inference among a set
of variables, modeled using a directed acyclic graph. However, one often does not
have the network, but only a set of observations, and wants to reconstruct the
network that generated the data. The \Rpackage{bnstruct} package provides objects and methods for
learning the structure and parameters of the network, also in presence of missing data,
and a set of additional tools to use Bayesian Networks, such as methods to perform
belief propagation.

This document is intended to show some examples of how \Rpackage{bnstruct} can be used
to learn and use Bayesian Networks. First we describe how to manage data sets,
how to use them to discover a Bayesian Network, and finally how to perform some
operations on a network.

Complete reference for classes and methods can be found in the package documentation.

\section{Installation}

The latest version of \Rpackage{bnstruct} can be found at \url{http://github.com/sambofra/bnstruct}.

In order to install the package, it suffices to launch\\
\verb!R CMD INSTALL path/to/bnstruct!\\
from a terminal, or to use R command \verb!install_packages!.

Being hosted on GitHub, it is also possible to use Hadley Wickham's \texttt{install\_github} tool from
an R session:
<<installgithub,eval=FALSE>>=
library("devtools")
install_github("sambofra/bnstruct")
@

\Rpackage{bnstruct} requires R $\geq 2.10$,
and depends on \Rpackage{bitops}, \Rpackage{igraph}, \Rpackage{Matrix} and \Rpackage{methods}.
Package \Rpackage{Rgraphviz} is requested in order to plot graphs, but it is not mandatory.


\section{Data sets}
The class that \Rpackage{bnstruct} provides to manage datasets is \Robject{BNDataset}.
It contains all of the data and the informations related to it: raw and imputed data, raw and imputed
bootstrap samples, and variable names and cardinality.

<<bndataset.constructor>>=
dataset <- BNDataset(name="Example")
# creates an enpty BNDataset object
@


\subsection{Data format}
\Rpackage{bnstruct} requires the data files to be in a format we describe in this section. 
The actual data has to be in (a text file containing data in) tabular format, one tuple per row,
with the values for each variable separated by a space or a tab. Values for each variable have to be
numbers, starting from \texttt{0} in case of discrete variables.
Data files can have a first row containing the names of the corresponding variables.

In addition to the data file, a header file containing additional informations can also be provided.
An header file has to be composed by three rows of tab-delimited values:
\begin{enumerate}
\item list of names of the variables, in the same order of the data file;
\item a list of integers representing the cardinality of the variables, in case of discrete variables,
      or the number of levels each variable has to be quantized in, in case of continuous variables;
\item a list that indicates, for each variable, if the variable is continuous (\texttt{c} or \texttt{C}), and thus has to be quantized before learning,
 or discrete (\texttt{d} or \texttt{D}).
\end{enumerate}

We provide two sample datasets, one with complete data (the \texttt{Asia} network) and one with missing values
(the \texttt{Child} network), in the \texttt{extdata} subfolder;
the user can refer to them as an example.

\subsection{Importing a dataset}
The preferred way to create a \Rclass{BNDataset} object is by reading a dataset from a file.
In order to accomplish this, we provide the \Rmethod{read.dataset} method.

<<read.dataset,eval=FALSE>>=
dataset <- BNDataset(name="Example")
dataset <- read.dataset(dataset,
                        header.file = "path/to/file.header",
                        data.file   = "path/to/file.data")
@

The sample datasets we provide come with two custom loaders:
<<sample.datasets>>=
asia.data  <- asia()
child.data <- child()
@

\subsection{Creating a \Robject{BNDataset} from data}
Another possible way for creating a \Robject{BNDataset} is to create it from a \Rclass{data.frame} or a \Rclass{matrix}
and some metadata. This is useful, for example, for instantiating a dataset after the data has already been 
processed in some way.

In particular, it is requested to provide the data, in \texttt{data.frame} or \texttt{matrix} form, and three additional
vectors of informations on the domain: one containing the names of the variables, another one containing values indicating
the cardinality (for discrete variables) or the quantization domain (for continuous variables) of the variables,
and the last one containing the status of the variables (\texttt{c} for continuous, \texttt{d} for discrete). Please note that
all of the metadata are required when choosing this option, and it is also suggested that the slot names
are specified when passing data and metadata as parameters to the constructor;
when no dataset name is provided, the slot names are mandatory.

<<bndataset2,eval=FALSE>>==
data <- matrix(c(1:16), nrow = 4, ncol = 4)
dataset <- BNDataset(name = "MyData", data = data,
                     variables = c("a", "b", "c", "d"),
                     node.sizes = c(4,8,12,16),
                     discreteness = rep('d',4))
@

\subsection{Imputation}
A dataset may contain various kinds of missing data, namely unobserved variables,
and unobserved values for otherwise observed variables. We currently deal only with this
second kind of missing data. The process of guessing the missing values is called \emph{imputation}.

We provide the \Rmethod{impute} function to perform imputation.
<<imputation1,eval=FALSE>>=
dataset <- BNDataset(name="Example")
dataset <- read.dataset(dataset,
                        header.file = "path/to/file.header",
                        data.file   = "path/to/file.data")
dataset <- impute(dataset)
@

Imputation is accomplished with the k-Nearest Neighbour algorithm. The number of neighbours to be used
can be chosen specifying the \Rfunarg{k.impute} parameter.
Imputation can also be performed during the loading of a dataset, as shown in the following example.
<<imputation2,eval=FALSE>>=
dataset <- BNDataset(name="Example")
dataset <- read.dataset(dataset,
                        header.file = "path/to/file.header",
                        data.file   = "path/to/file.data",
                        imputation  = TRUE,
                        k.impute    = 10)
@

Note that, when imputed data is present, it has higher priority over raw data when using a dataset
(see section \ref{sec:usingdata}).

The sample dataset available using the \Rmethod{child()} method contains both raw and imputed data.

\subsection{Bootstrap}
\Robject{BNDataset} objects have also room for bootstrap samples, i.e. random samples with replacement of the original data with the same number of observations, both for raw and imputed data.
We provide the \Rmethod{bootstrap} method for this.
<<bootstrap1,eval=FALSE>>=
dataset <- BNDataset(name="Example")
dataset <- read.dataset(dataset,
                        header.file = "path/to/file.header",
                        data.file   = "path/to/file.data")
dataset <- bootstrap(dataset, num.boots = 100)
dataset.with.imputed.samples <- bootstrap(dataset,
                            num.boots = 100, imputation = TRUE)
@

Again, the generation of bootstrap samples can be performed while loading a dataset.
<<bootstrap2,eval=FALSE>>=
dataset <- BNDataset(name="Example")
dataset <- read.dataset(dataset,
                        header.file = "path/to/file.header",
                        data.file   = "path/to/file.data",
                        bootstrap   = TRUE,
                        num.boots   = 100,
                        imputation  = TRUE)
@

The sample datasets provided have no bootstrap samples in them.

\subsection{Using data}
\label{sec:usingdata}
After a \Robject{BNDataset} has been created, it is ready to be used.
The complete list of methods available for a \Robject{BNDataset} object is available
in the package documentation; we are not going to cover all of the methods in this brief series
of examples, but we just show how to retrieve data.

The main operation that can be done with a \Robject{BNDataset} is to get the data it contains.
The main methods we provide are \Rmethod{get.raw.data}, \Rmethod{get.imputed.data} and \Rmethod{get.data}. \Rmethod{get.data} is just a proxy
for one of the other two methods.
As previously mentioned, imputed data (if present) has higher priority over raw data, since it is supposed to be more useful.
Therefore, if imputed data is present, \Rmethod{get.data} will behave as \Rmethod{get.imputed.data}; otherwise,
it will return the raw dataset just like \Rmethod{get.raw.data}.

<<getdata1,eval=FALSE>>=
dataset.1 <- child()
# if we want raw data
get.raw.data(dataset.1)
# if we want imputed dataset, the following are equivalent
get.imputed.data(dataset.1)
get.data(dataset.1)
@

<<getdata1.1,eval=FALSE>>=
dataset.2 <- asia()
# we can only get raw data, the following are equivalent
get.raw.data(dataset.2)
get.data(dataset.2)
@

We can check if a dataset has imputed data or not with the \Rmethod{has.imputed.data} method.
<<getdata2,eval=FALSE>>=
dataset.1 <- child()
has.imputed.data(dataset.1) # TRUE
@

<<getdata2.1,eval=FALSE>>=
dataset.2 <- asia()
has.imputed.data(dataset.2) # FALSE
@

In order to retrieve bootstrap samples, one can use the \Rmethod{boots} and \Rmethod{imp.boots} methods for samples
made of raw and imputed data. The presence of imputed samples can be tested using \Rmethod{has.imp.boots}.
We also provide the \Rmethod{get.boot} method to directly access a single sample. Again, imputed samples have higher priority.
<<getboot,eval=FALSE>>=
# get imputed samples
for (i in 1:num.boots(dataset))
  print( get.boot(dataset, i) )
@

<<getboot.1,eval=FALSE>>=
# get raw samples
for (i in 1:num.boots(dataset))
  print( get.boot(dataset, i, imputed = FALSE) )
@


\section{Bayesian Networks}
Bayesian Network are represented using the \Robject{BN} object. It contains information regarding the variables in the network,
the directed acyclic graph (DAG) representing the structure of the network, the conditional probability tables entailed by 
the network, and the weighted partially DAG representing the structure as learnt using bootstrap samples.

<<bn1>>=
net <- BN(name = "Example")
@


The method of choice to create a \Robject{BN} object is to create it from a \Robject{BNDataset}.
The following code will create an empty \Robject{BN} object for the \texttt{Child} network.
<<bn2, eval=FALSE>>=
dataset <- child() # or any other way to create a custom BNDataset
net     <- BN(dataset)
@

% Note that the above snippet already creates a full object, that is, a network with structure and parameters.
% To better highlight the various options and possible operations, we delve a little into the details
% of the main learning methods.

Now, starting from the empty network and the dataset, we can proceed with the tasks of structure and parameter learning.

\subsection{Structure learning}
When constructing a network starting from a dataset, the first operation we may want to perform is to learn
the structure of the network. \Rpackage{bnstruct} provides the \Rmethod{learn.structure} method for this task.
<<structurelearn1, eval=FALSE>>=
dataset <- child() # or any other way to create a custom BNDataset
net     <- BN(dataset)
net     <- learn.structure(net, dataset)
@

The \Rmethod{learn.structure} method returns a new \Robject{BN} object, with a new DAG (or WPDAG, if the structure
learning has been performed using bootstrap -- more on this later).

We provide two algorithms in order to learn the structure of the network, that can be chosen with the \texttt{algo} parameter.
The first is the Silander-Myllym\"aki (\texttt{sm})
exact search-and-score algorithm (see \citet*{silander2012simple}), that performs a complete evaluation of the search space in order to discover
the best network; this algorithm may take a very long time, and can be inapplicable when discovering networks
with more than 25--30 nodes. Even for small networks, users are strongly encouraged to provide
meaningful parameters such as the layering of the nodes, or the maximum number of parents -- refer to the 
documentation in package manual for more details on the method parameters.

The second algorithm (and the default one) is the Max-Min Hill-Climbing heuristic (\texttt{mmhc}, see \citet*{tsamardinos2006max}), that performs a statistical
sieving of the search space followed by a greedy evaluation. It is considerably faster than the complete method, at the cost of a (likely)
lower quality. Also note that in the case of a very dense network and lots of obsevations, the statistical evaluation
of the search space may take a long time. Also for this algorithm there are parameters that may need to be tuned,
mainly the confidence threshold of the statistical pruning.

Search-and-score methods also need a scoring function to compute an estimated measure of each configuration of nodes.
We provide three of the most popular scoring functions, \texttt{BDeu} (Bayesian-Dirichlet equivalent uniform, default),
\texttt{AIC} (Akaike Information Criterion) and \texttt{BIC} (Bayesian Information Criterion). The scoring function
can be chosen using the \texttt{scoring.func} parameter.

<<structurelearn2, eval=FALSE>>=
dataset <- child() # or any other way to create a custom BNDataset
net     <- BN(dataset)
net.1   <- learn.structure(net, dataset,
                           algo = "sm", scoring.func = "AIC")
net.2   <- learn.structure(net, dataset,
                           algo = "mmhc", scoring.func = "BDeu")
@

% Since building a \Robject{BN} object from a \Robject{BNDataset} already performs a structure learning task,
% we can specify the parameters of the \Rmethod{learn.structure} method in the \Rmethod{BN} constructor.
% # <<structurelearn3, eval=FALSE>>=
% # dataset <- child() # or any other way to create a custom BNDataset
% # net     <- BN(dataset, algo = "sm", scoring.func = "AIC")
% # @

The \Rmethod{learn.structure} method by default computes the structure as a DAG. We can however use
bootstrap samples to learn a weighted partially DAG, in order to get a weighted confidence on the presence or
absence of an edge in the structure (\citet*{friedman1999data}). This can be done by providing the constructor or the \Rmethod{learn.structure}
method a \Robject{BNDataset} with bootstrap samples, and the additional parameter \Rfunarg{bootstrap = TRUE}.
<<structurelearn4, eval=FALSE>>=
dataset <- asia() # or any other way to create a custom BNDataset
net     <- BN(dataset)
net.1   <- learn.structure(net, dataset, algo = "mmhc",
              scoring.func = "AIC", bootstrap = TRUE)
# or, for explicitely learning from raw data
net.2   <- learn.structure(net, dataset, algo = "mmhc",
              scoring.func = "AIC", bootstrap = TRUE,
              imputation = FALSE)
@

\subsection{Parameter learning}
Parameter learning is the operation that learns the conditional probabilities entailed by a network,
given the data and the structure of the network. \Rpackage{bnstruct} provides the \Rmethod{learn.params} method for this task,
performing a Maximum-A-Posteriori (MAP) estimate of the parameters.
% Also, parameter learning is already performed by the constructor when fed with a \Robject{BNDataset}, and optional
% parameters for \Rmethod{learn.params} such as \Rfunarg{ess} can be passed also to the network constructor.
<<paramlearn, eval=FALSE>>=
dataset <- asia() # or any other way to create a custom BNDataset
net     <- BN(dataset)
net     <- learn.structure(net, dataset, 
                           algo = "mmhc",
                           scoring.func = "AIC")
net     <- learn.params(net, dataset)
@

\section{Using a network}
Once a network is created, it can be used. Here we briefly mention some of the basic methods provided in order to
manipulate a network and access its components.

First of all, it is surely of interest to obtain the structure of a network. The \Rpackage{bnstruct} package
provides the \Rmethod{dag()} and \Rmethod{wpdag()} methods in order to access the structure of a network learnt without and
with bootstrap (respectively).
<<dag1,eval=FALSE>>=
dag(net)
wpdag(net.boot)
@

Then we may want to retrieve the parameters, using the \Rmethod{cpts()} method.
<<cpts2,eval=FALSE>>=
cpts(net)
@

Another common operation that we may want to perform is displaying the network, or printing its main informations, using the
\Rmethod{plot()}, \Rmethod{print()} and \Rmethod{show()} methods. Note that the \Rmethod{plot()} method is flexible enough to allow 
some custom settings such as the choice of the colors of the nodes, and, more importantly, some threshold settings 
for the networks learnt with bootstrap. As default, the DAG of a network is selected for plotting, if available,
otherwise the WPDAG is used. In case of presence of both the DAG and the WPDAG, in order to specify the latter as
structure to be plotted, the \Rfunarg{plot.wpdag} logical parameter is provided.
<<plotprint,eval=FALSE>>=
print(net)
plot(net) # regular DAG
plot(net, plot.wpdag=T) # wpdag
plot(net.boot)
@

The \Rmethod{show()} method is an alias for the \Rmethod{print()} method, but allows to print the state of an instance of an object
just by typing its name in an \texttt{R} session.
<<show,eval=FALSE>>=
# TFAE
print(net)
show(net)
net
@


\section{Inference in networks}
\subsection{Belief Propagation}
The \Rpackage{bnstruct} package provides a tool to perform belief propagation using a junction tree.
This tool is the \Robject{InferenceEngine} object.
It contains a copy of a network, an updated network, the adjacency matrix of the junction tree computed
starting from the original network, the list of cliques of variables that form the nodes of the junction tree
and the list of joint probability tables for the cliques composing the junction tree.

An \Robject{InferenceEngine} can be built from a network: in this case, the junction tree is immediately constructed.
%Alternatively, one can use the \Rmethod{build.junction.tree} method.
<<infeng1, eval=FALSE>>=
dataset <- asia() # or any other way to create a custom BNDataset
net     <- BN(dataset)
inf.eng <- InferenceEngine(net)
@
% # equivalent to
% inf.eng <- InferenceEnfine()
% bn(inf.eng) <- net
% build.junction.tree(inf.eng, dag(net))
% @

Belief propagation over the junction tree can be then performed using the \Rmethod{belief.propagation} method.
<<belprop1, eval=FALSE>>=
dataset <- asia() # or any other way to create a custom BNDataset
net     <- BN(dataset)
inf.eng <- InferenceEngine(net)
inf.eng <- belief.propagation(inf.eng)
@

Belief propagation can be fed with a list of observations. This can be done in two ways: as parameters in the method,
or inserting them directly into the inference engine. Note that the two options are mutually exclusive, in the sense that
the list of observations given as parameter replaces (in the whole \Robject{InferenceEngine} object returned)
the observations contained in the engine. Furthermore, if a list of observations contains multiple observations
of the same variable, only the last one is considered.
<<belprop2, eval=FALSE>>=
dataset <- asia() # or any other way to create a custom BNDataset
net     <- BN(dataset)
net     <- learn.structure(net, dataset)
net     <- learn.params(net, dataset)
inf.eng <- InferenceEngine(net)
inf.eng <- belief.propagation(inf.eng,
                              observed.vars = c("Asia","X-ray"),
                              observed.vals = c(1,1))
print(updated.bn(inf.eng))
# is equivalent to
observations(inf.eng) <- list(c("Asia","X-ray"), c(1,1))
inf.eng <- belief.propagation(inf.eng)
plot(updated.bn(inf.eng))
@

\subsection{The Expectation-Maximization algorithm}
\Rpackage{bnstruct} can also use an \Robject{InferenceEngine} and a \Robject{BNDataset} to perform
the Expectation-Maximization algorithm to estimate the parameters of the network.
It suffices to use the \Rmethod{em} method, that returns an \Robject{InferenceEngine} containing
an updated network with the newly estimated conditional probability tables.
<<em1,eval=FALSE>>=
dataset <- child() # or any other way to create a custom BNDataset
net     <- BN(dataset)
net     <- learn.structure(net, dataset)
net     <- learn.params(net, dataset)
inf.eng <- InferenceEngine(net)
inf.eng <- em(inf.eng, dataset)
plot(updated.bn(inf.eng))
@

\bibliographystyle{plainnat}
\bibliography{bibtex}

\end{document}
